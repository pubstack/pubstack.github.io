---
layout: post
title: "Kubeflow 101: Deployment and usage"
author: "Carlos Camacho"
categories:
  - blog
tags:
- kubernetes
- kubeinit
- openshift
- kubeflow
- mlops
favorite: false
commentIssueId: 91
refimage: '/static/kubeflow/arch.png'
---

# TL;DR;

This is a copy/paste guide to get you a fully working Kubeflow development environment.

## Install requirements
We will install some requirements to deploy the infrastructure required to support kubeflow 2.0.0

```
# Install the requirements assuming python3/pip3 is installed
pip3 install \
        --upgrade \
        pip \
        shyaml \
        ansible \
        netaddr

# Get the project's source code
git clone https://github.com/Kubeinit/kubeinit.git
cd kubeinit

# Install the Ansible collection requirements
ansible-galaxy collection install --force --requirements-file kubeinit/requirements.yml

# Build and install the collection
rm -rf ~/.ansible/collections/ansible_collections/kubeinit/kubeinit
ansible-galaxy collection build kubeinit --verbose --force --output-path releases/
ansible-galaxy collection install --force --force-with-deps releases/kubeinit-kubeinit-`cat kubeinit/galaxy.yml | shyaml get-value version`.tar.gz
```

## Deploy a 1 singlenode 4.13 OKD development environment

This step will get us a single-node OpenShift 4.13 development environment.

```
# Run the playbook
ansible-playbook \
    -v --user root \
    -e kubeinit_spec=okd-libvirt-1-0-1 \
    -e hypervisor_hosts_spec='[{"ansible_host":"nyctea"},{"ansible_host":"tyto"}]' \
    -e controller_node_disk_size='100G' \
    -e controller_node_ram_size='88080384' \
    ./kubeinit/playbook.yml
```

## Configuring the storage PV backend

We will setup some basic default storage class to support the PVC from the kubeflow deploymnet.

```
# Create a new namespace
oc new-project nfsprovisioner-operator

# Deploy NFS Provisioner operator in the terminal
cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nfs-provisioner-operator
  namespace: openshift-operators
spec:
  channel: alpha
  installPlanApproval: Automatic
  name: nfs-provisioner-operator
  source: community-operators
  sourceNamespace: openshift-marketplace
EOF

# We assume this is a single node deployment
export target_node=$(oc get node --no-headers -o name|cut -d'/' -f2)
oc label node/${target_node} app=nfs-provisioner

```

Now we need to configure the folder where the PVs will be stored.

```
# ssh to the node
oc debug node/${target_node}

# Create a directory and set up the Selinux label.
chroot /host
mkdir -p /home/core/nfs
chcon -Rvt svirt_sandbox_file_t /home/core/nfs
exit; exit

```

Create the NFSProvisioner Custom Resource

```
cat << EOF | oc apply -f -  
apiVersion: cache.jhouse.com/v1alpha1
kind: NFSProvisioner
metadata:
  name: nfsprovisioner-sample
  namespace: nfsprovisioner-operator
spec:
  nodeSelector: 
    app: nfs-provisioner
  hostPathDir: "/home/core/nfs"
EOF

# Check if NFS Server is running
oc get pod

# Update annotation of the NFS StorageClass
oc patch storageclass nfs -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

# Check the default next to nfs StorageClass
oc get sc
NAME            PROVISIONER       RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs (default)   example.com/nfs   Delete          Immediate           false                  4m29s

```

Create a test PVC:

```

# Create a test PVC
oc apply -f https://raw.githubusercontent.com/Jooho/jhouse_openshift/master/test_cases/operator/test/test-pvc.yaml
persistentvolumeclaim/nfs-pvc-example created

# Check the test PV/PVC
oc get pv,pvc

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                 STORAGECLASS   REASON   AGE
persistentvolume/pvc-e30ba0c8-4a41-4fa0-bc2c-999190fd0282   1Mi        RWX            Delete           Bound       nfsprovisioner-operator/nfs-pvc-example               nfs                     5s

NAME                                    STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/nfs-pvc-example   Bound    pvc-e30ba0c8-4a41-4fa0-bc2c-999190fd0282   1Mi        RWX            nfs            5s

```
The output shown here indicates that the NFS server, NFS provisioner, and NFS StorageClass are all working fine. You can use the NFS StorageClass for any test scenarios that need PVC.

## Deploy the Kubeflow templates

```   
# Installing the Kubeflow templates
# https://www.kubeflow.org/docs/components/pipelines/v1/installation/localcluster-deployment/#deploying-kubeflow-pipelines
export PIPELINE_VERSION=2.0.0
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION"
kubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io
kubectl apply -k "github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref=$PIPELINE_VERSION"
```

## Checking that all the kubeflow services are running

```
[root@service ~]# oc get pods -n kubeflow
NAME                                               READY   STATUS    RESTARTS      AGE
cache-deployer-deployment-76f8bc8897-t48vs         1/1     Running   0             3m
cache-server-65fc86f747-2rg7t                      1/1     Running   0             3m
metadata-envoy-deployment-5bf6bbb856-tqw85         1/1     Running   0             3m
metadata-grpc-deployment-784b8b5fb4-l94tm          1/1     Running   3 (52s ago)   3m
metadata-writer-647bfd9f77-m5c8w                   1/1     Running   0             3m
minio-65dff76b66-vstbk                             1/1     Running   0             3m
ml-pipeline-86965f8976-qbgqs                       1/1     Running   0             3m
ml-pipeline-persistenceagent-dbc9d95b6-g7nsb       1/1     Running   0             3m
ml-pipeline-scheduledworkflow-6fbf57b54d-446f5     1/1     Running   0             2m59s
ml-pipeline-ui-5b99c79fc8-2vbcp                    1/1     Running   0             2m59s
ml-pipeline-viewer-crd-5fdb467bb5-rktvs            1/1     Running   0             2m59s
ml-pipeline-visualizationserver-6cf48684f5-b929v   1/1     Running   0             2m59s
mysql-c999c6c8-jzths                               1/1     Running   0             2m59s
workflow-controller-6c85bc4f95-lmkrg               1/1     Running   0             2m59s
```



## Jumping in the web UI
```
# From your machine connect to the hypervisor
ssh -L 38080:localhost:38080 root@labserver
ssh -L 38080:localhost:8088 -i ~/.ssh/okdcluster_id_rsa root@10.0.0.253
# Connect to the UI pod
kubectl port-forward -n kubeflow svc/ml-pipeline-ui 8088:80
```

![](/static/kubeflow/kubeflow_ui.png)

### Conclusions

Deploying also OpenShift demostrates how
flexible KubeInit can be.
With a few changes we can deploy also downstream Kubernetes
distributions ready to be used for production grade deployments.

Once [#219](https://github.com/Kubeinit/kubeinit/pull/219/files)
is merged you should be able to run this.

### The end

If you like this post, please try the code, raise issues, and ask for more details, features or
anything that you feel interested in. Also it would be awesome if you become a stargazer to catch up
updates and new features.

This is the main project [repository](https://github.com/kubeinit/kubeinit).

Happy KUbeflowing/KubeIniting!
